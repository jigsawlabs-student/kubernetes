{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Docker to Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now so far, we have learned about Docker.  As we know, Docker is our tool for building images and running containers. By using containers we keep our various services in an application independent and isolated.  This isolation ensures that our tasks are self-contained -- everything needed to run the service is inside our container.  \n",
    "\n",
    "But what Docker is not particularly strong at is deploying these containers, and especially across different computers.  For that, we'll be using Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Kubernetes Helps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far with Docker, deploying our containers has been relatively simple.  For one, we've only had a few containers to deploy and we have deployed these containers on one or two computers.  But in a production application, we may be working with hundreds of containers, and these containers can be spread across dozens of different computers.  \n",
    "\n",
    "\n",
    "* **Updates**\n",
    "\n",
    "Consider what it means if we have this setup and one of those computers houses the container that is running our Flask Api and we want to update the version of Flask.  After we update our codebase and DockerFile, we'll then need to find that particular computer that houses that container, and redeploy it.  Keeping track of which computer is time consuming.  With Kubernetes, it will find the container on whichever computer it lives and apply the update.\n",
    "\n",
    "While this may seem like a small task, now think about what would be involved if we have many containers running our Flask API across different computers.  We would have to find each container and redeploy the container on each computer.  \n",
    "\n",
    "With Kubernetes, we can just specify the type of container to update, and Kubernetes will find the containers wherever they live.\n",
    "\n",
    "* **Replicas**\n",
    "\n",
    "This brings us to replicas.  When we have multiple containers of the same image running, this is a replica.  Why would we replicate something like our Flask Api, well if the first one becomes overburdened with requests from users.  But where should we place this new container?  We need to choose a machine that has the resources, whether hard drive or memory or CPU, to run the container.  We could just automatically create a new computer and deploy the container, but this would be wasteful.  As you can imagine, determining which computer can handle our new containers is a tricky process, that can change as other containers become overburdened.\n",
    "\n",
    "With Kubernetes, we won't worry about any of this.  Kubernetes will keep track of our available resources across our cluster of computers and determine where each new container goes.  And if there is not enough across our current computers, Kubernetes can will add a new computer for us (via autoscaling).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Self-Healing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to consider is that our AWS machines can sometimes fail.  For example, sometimes a cloud provider like AWS will update the underlying machine which will cause downtime.  Or there can be some sort of issue that takes the machines offline for some period of time.  If any of the machines fail, and the containers they run on are thus not available, Kubernetes will move the containers to some working machine.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A new way of thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now perhaps we can see the problem that Kubernetes solves: while we a dynamic number of computers running to properly scale the various services in our application, it becomes a lot easier if we don't need to consider exactly what service is running on which machine.  Kubernetes allows us to just consider what processes run on our *cluster* of machines.  We don't care who runs what.  \n",
    "\n",
    "If we need to update a particular service, Kubernetes will find the machines where it is running to provide the updates.  If we need to reproduce a service, Kubernetes will decide which computer has the resources for this task, or allocate additional resources if need be.\n",
    "\n",
    "Notice that this is the same way that we have been thinking of software development thus far.  We generally don't care which CPU runs what our different computer applications, or which location in memory is used to temporarily store some data.  The same should be when were working with multiple computers.\n",
    "\n",
    "This is called thinking of the cluster as an operating system.  Just like the operating system decides where to allocate resources when working with a single computer.  We don't really worry about the hardware too much, we just worry about the software.  Kubernetes has the aim of allowing us to keep this mindset.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned of the issues that Kubernetes can help us solve when working with many containers across multiple different machines.  Kubernetes determines which machines have the resources needed to run which containers, and then places the containers there accordingly.  When we need to provide updates to our containers, Kubernetes finds where the relevant containers are currently running to apply the updates.  And if a container shutsdown because the machine it's running on becomes unavailable, Kubernetes will find a new machine to create the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These services allow us offload the task of considering which computer resources in our cluster to allocate to each container.  Instead, we can think of what containers and replicas of containers we want run on our cluster in general, and allow Kubernetes to consider the particular location for each task.  In this way, Kubernetes acts as the operating system: it allocates resources for tasks across our cluster in the same way an operating system allocates CPU or memory in a single computer for various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[K8s autoscaling](https://www.replex.io/blog/kubernetes-in-production-best-practices-for-cluster-autoscaler-hpa-and-vpa)\n",
    "\n",
    "[K8s Self-Healing](https://www.youtube.com/watch?v=91dgNqma7-Q&ab_channel=CNCF%5BCloudNativeComputingFoundation%5D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
